#! /usr/bin/env perl

use strict;
use Carp;
use Getopt::Long qw(:config no_ignore_case);

$| = 1;   # auto flush

my $usage = <<"End_of_Usage";
Usage: $0 [options] qsub_walltime output_directory

Options:
        -A alloc         - project allocation for the qsub -A parameter
        -c chunk         - number of nodes per qsub job (D = 5)
        -dry             - dry run
        -n nodes         - max total nodes to use (D = 10)
        -N name          - job name for the qsub -N parameter
        -q queue         - job queue for the qsub -q parameter
        -s env           - environment setting file to source 
        -t threads       - number of threads each command uses (D = 1)
        -redo            - overwrite project directory

Examples:

  > for i in {1..60}; do echo "echo \$i; sleep 60"; done | cpar -t 8 -n 10 -c 5 3:00 test

    The first part of this pipe tries to launch 60 jobs each sleeping
    for one minute.  With "-c 5", we parallelize it by launching qsub
    jobs each using 5 nodes.  The "-t 8" specifies 8 cores to be
    allocated for each command; thus, on a machine with 24-core nodes,
    each aprun line will execute (24/8=) 3 commands on the same
    node. This means our 5-node qsub can handle at least (5*3=) 15
    commands. Since we are allowed to use as many as 10 nodes (-n), we
    will submit two such qsubs, which gives us a throughput of 30
    commands in one iteration. The cpar tool will thus instruct each
    qsub to have two iterations (10 aprun lines). The completion time
    is thus 2 * 60 seconds plus overhead. Our 3-minute allocation
    should be sufficient.

  > find /path/to/query -name *.fasta | parallel --dry-run blastall -a 6 -p blastp -d /NR/nr -i {} | cpar -t 6 1:00:00 output

    This pipeline will launch 1 hour jobs on no more than 10 nodes to
    blast every query file against the NR. A large number of query files
    will result in multiple sequential aprun lines on each node. So the
    qsub job length needs to be adjusted manually to reflect that.

End_of_Usage

my ($help, $alloc, $chunk, $dry, $name, $nproc, $nthread, $queue, $redo, $source, $tmpdir);

GetOptions("h|help"     => \$help,
           "A|alloc=s"  => \$alloc,
           "c|chunk=i"  => \$chunk,
           "dry"        => \$dry,
           "n|np=i"     => \$nproc,
           "N|name=s"   => \$name,
           "q|queue=s"  => \$queue, 
           "r|redo"     => \$redo,
           "s|source=s" => \$source,
           "t|nt=i"     => \$nthread,
	  ) or die("Error in command line arguments\n");

$help and die $usage;

$chunk   ||= 5;
$nproc   ||= 10;
$nthread ||= 1;
$name    ||= "cpar";
$chunk     = min($nproc, $chunk);

my $wall = shift @ARGV or die $usage;
my $dir  = shift @ARGV or die $usage;

-d $dir && !$redo and die "Directory '$dir' exists.\n\n$usage";
remove_dir($dir) if $redo;
verify_dir($dir);

my @cmds = map { chomp; $_ } <STDIN>;

my $cores_per_node = 24;
my $jobs_per_node  = int($cores_per_node / $nthread);

my $i = 0;
my $ci = 0;
while (@cmds > 0) {
    $i++; 
    my $jobfile = "$dir/node_n$i.sh";
    open(F, ">$jobfile") or die "Could not open $jobfile";
    print F "#!/bin/bash\n\n";
    print F 'time__="\nCommand: %C\n%Uuser %Ssystem %Eelapsed %PCPU (%Xtext+%Ddata %Mmax)k\n%Iinputs+%Ooutputs (%Fmajor+%Rminor)pagefaults %Wswaps\n"'."\n";
    print F "host__=`hostname`\n\n";
    print F 'echo $host__ "BEG> " `date`'."\n\n";
    # print F "module swap PrgEnv-cray PrgEnv-gnu\n\n";

    for (my $j = 1; $j <= $jobs_per_node; $j++) {
        $ci++;
        
        my $cmd = shift @cmds or last;
        my $cmdfile = "cmd_$ci.sh";
        my $cmdlog = "out_cmd_$ci.txt";
        create_cmd_file($cmd, "$dir/$cmdfile", $cmdlog);
        print F "(/usr/bin/time -a -o time_n$i.txt -f \"\$time__\" ./cmd_$ci.sh && echo $ci >>DONE.cmds) &\n";
        # print F "(/usr/bin/time cmd_$ci.sh && echo $ci >>DONE.cmds) &\n";
        # print F "(time cmd_$ci.sh && echo $ci >>DONE.cmds) &\n";
    }

    print F "\nwait\n\n";
    print F 'echo $host__ "END> " `date`'."\n\n";
    close(F);
    run("chmod a+x $jobfile");
}
my $tot = $i; $nproc = min($nproc, $tot);

chdir($dir);
my $beg = 1;
my $k = 1;
my $nodes = $nproc;                  # remaining nodes
while ($beg <= $tot) {
    my $left  = $tot - $beg + 1;     # remaining node-level jobs
    my $qsubs = min($nodes, $chunk); # nodes to use in the next qsub
    my $todo  = min(ceiling($left, ($nodes/$qsubs)), $left); # aprun lines in the qsub
    my $end   = $beg + $todo - 1;
    my $width = $qsubs * $cores_per_node;
    my $b2e   = $todo > 1 ? "$beg-$end" : $beg;
    my $range = "$b2e/$tot";
    my $pbs   = "job$k.pbs";

    open(F, ">$pbs") or die "Could not open $pbs";
    print F "#!/bin/bash\n\n";

    print F "#PBS -l walltime=$wall\n";
    print F "#PBS -l mppwidth=$width\n";
    print F "#PBS -N $name:$range\n";
    print F "#PBS -e $name-$b2e.".'$PBS_JOBID.err'."\n";
    print F "#PBS -o $name-$b2e.".'$PBS_JOBID.out'."\n\n";
    print F "source $source\n\n" if $source;
    print F 'cd $PBS_O_WORKDIR'."\n\n";

    if ($todo > 1) {
        print F "aprun ./node_n$_.sh &\n" for $beg..$end;
        print F "\nwait\n";
    } else {
        print F "aprun ./node_n$beg.sh\n";
    }
    close(F);

    my @cmd = ('qsub', $alloc ? "-A $alloc" : undef,
                       $queue ? "-q $queue" : undef, $pbs);

    system join(" ", @cmd) if !$dry;

    $beg = $end + 1; $k++;
    $nodes -= $qsubs;
}

sub create_cmd_file {
    my ($cmd, $file, $log) = @_;

    open(CMD, ">$file") or die "Could not open $file";

    print CMD "#!/bin/bash\n\n";
    print CMD "( $cmd ) > $log\n\n";
    
    close(CMD);
    run("chmod a+x $file");
}


sub ceiling { int(($_[0] - 1) / $_[1]) + 1; }
sub max { $_[0] > $_[1] ? $_[0] : $_[1]; }
sub min { $_[0] < $_[1] ? $_[0] : $_[1]; }
sub remove_dir { run("rm -rf $_[0]"); }
sub verify_dir { run("mkdir -p $_[0]");  }
sub run { system(@_) == 0 or confess("FAILED: ". join(" ", @_)); }
